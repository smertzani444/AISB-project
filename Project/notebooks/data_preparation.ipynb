{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb83491",
   "metadata": {},
   "source": [
    "# Imports & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1562cf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import os\n",
    "import shutil as sh\n",
    "import urllib\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import glob, os\n",
    "import importlib\n",
    "import gzip\n",
    "import MDAnalysis as mda\n",
    "import nglview as nv\n",
    "import requests\n",
    "import json\n",
    "from biopandas.pdb import PandasPdb\n",
    "from Bio import AlignIO\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "from urllib.error import HTTPError\n",
    "from pathlib import Path\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntProgress\n",
    "import ipywidgets as widgets # type: ignore\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "#Pandarallel works only on linux and mac\n",
    "try:\n",
    "    from pandarallel import pandarallel\n",
    "    pandarallel.initialize(nb_workers=8,progress_bar=True)\n",
    "    PARRALEL = True\n",
    "except:\n",
    "    PARRALEL = False\n",
    "\n",
    "from tqdm.notebook import tnrange, tqdm\n",
    "tqdm.pandas() #activate tqdm progressbar for pandas apply\n",
    "\n",
    "#Pandas configuration\n",
    "pd.options.mode.chained_assignment = (\n",
    "    None  # default='warn', remove pandas warning when adding a new column\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%config InlineBackend.figure_format ='svg' #better quality figure figure\n",
    "\n",
    "#%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f4f9f",
   "metadata": {},
   "source": [
    "# Folder Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79bfed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETUP = {} #Dictionnary with ALL parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99a3cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder definition\n",
    "from sys import platform\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    PEPRMINT_FOLDER = \"/home/user_stel/AISB/Project\"\n",
    "elif platform == \"darwin\":\n",
    "    PEPRMINT_FOLDER = \"/home/user_stel/AISB/Project\"\n",
    "else:\n",
    "    raise ValueError(\"OS NOT FOUND\")\n",
    "WORKDIR = f\"{PEPRMINT_FOLDER}/dataset/\"\n",
    "CATHFOLDER = f\"{PEPRMINT_FOLDER}/databases/cath/\"\n",
    "ALPHAFOLDFOLDER = f\"{PEPRMINT_FOLDER}/databases/alphafold/\"\n",
    "PROSITEFOLDER = f\"{PEPRMINT_FOLDER}/databases/prosite/\"\n",
    "UNIPROTFOLDER = f\"{PEPRMINT_FOLDER}/databases/uniprot/\"\n",
    "FIGURESFOLDER = f\"{PEPRMINT_FOLDER}/figures/\"\n",
    "\n",
    "SETUP[\"PEPRMINT_FOLDER\"]=PEPRMINT_FOLDER\n",
    "SETUP[\"WORKDIR\"]=WORKDIR\n",
    "SETUP[\"CATHFOLDER\"]=CATHFOLDER\n",
    "SETUP[\"PROSITEFOLDER\"]=PROSITEFOLDER\n",
    "SETUP[\"ALPHAFOLDFOLDER\"]=ALPHAFOLDFOLDER\n",
    "SETUP[\"UNIPROTFOLDER\"]=UNIPROTFOLDER\n",
    "SETUP[\"FIGURESFOLDER\"]=FIGURESFOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f177eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PEPRMINT_FOLDER):\n",
    "    os.makedirs(PEPRMINT_FOLDER)\n",
    "if not os.path.exists(WORKDIR):\n",
    "    os.makedirs(WORKDIR)\n",
    "if not os.path.exists(FIGURESFOLDER):\n",
    "    os.makedirs(FIGURESFOLDER)\n",
    "if not os.path.exists(ALPHAFOLDFOLDER):\n",
    "    os.makedirs(ALPHAFOLDFOLDER)\n",
    "if not os.path.exists(UNIPROTFOLDER):\n",
    "    os.makedirs(UNIPROTFOLDER)\n",
    "if not os.path.exists(PROSITEFOLDER):\n",
    "    os.makedirs(PROSITEFOLDER) #MSA will contains the alignments in \"msa\" format (FASTA). \n",
    "if not os.path.exists(CATHFOLDER):\n",
    "    os.makedirs(CATHFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d85d9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in SETUP:\n",
    "    exec(f\"{k}2 = SETUP['{k}']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2290a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN_PROSITE = {\n",
    "    \"PH\": \"PS50003\",\n",
    "    \"C2\": [\"PS50004\",\"PS51547\"],\n",
    "    \"C1\": \"PS50081\",  # Note : no C1 prosite on SMART but 2 C1 ProSite on Interprot (PS50081,PS00479), I took PS50081 since the data in PS00479 are in PS50081.\n",
    "    \"PX\": \"PS50195\",\n",
    "    # \"FYVE\":\"PS50178\",\n",
    "    \"FYVE\": [\"PS50178\",'PS50089', 'PS00518','PS50016','PS01359','PS50014','PS00633','PS50119'],  # FYVE CAN BE THIS ONE TOO....\n",
    "    # \"PPASE_MYOTUBULARIN\":\"PS51339\",# no GRAM domain found on prosite. Has to do this manually. Go on http://smart.embl-heidelberg.de/smart/do_annotation.pl?DOMAIN=GRAM&BLAST=DUMMY\n",
    "    \"BAR\": \"PS51021\",  # 1URU is missing on prosite\n",
    "    # \"GLA\":\"PS50963\",\n",
    "    \"ENTH\": \"PS50942\",\n",
    "    \"SH2\": \"PS50001\",\n",
    "    \"SEC14\": \"PS50191\",\n",
    "    \"START\": \"PS50848\",\n",
    "    \"C2DIS\":\"PS50022\",\n",
    "    \"GLA\": \"PS50998\",\n",
    "    \"PLD\":\"PS50035\",\n",
    "    \"PLA\":\"PS00118\",\n",
    "    \"ANNEXIN\":\"PS00223\",\n",
    "}\n",
    "# Invert keys and values to have PROSITEID ==> DOMAIN\n",
    "PROSITE_DOMAIN = {}\n",
    "for key, value in DOMAIN_PROSITE.items():\n",
    "    if type(value) == type([]):\n",
    "        for subvalues in value:\n",
    "            PROSITE_DOMAIN[subvalues] = key\n",
    "    else:\n",
    "        PROSITE_DOMAIN[value] = key\n",
    "# PROSITE_DOMAIN = {v: k for k, v in DOMAIN_PROSITE.items()}\n",
    "\n",
    "DOMAIN_CATH = {\n",
    "    \"PH\": \"2.30.29.30\",\n",
    "    \"C2\": \"2.60.40.150\",\n",
    "    \"C1\": \"3.30.60.20\",\n",
    "    \"PX\": \"3.30.1520.10\",\n",
    "    \"FYVE\": \"3.30.40.10\",\n",
    "    \"BAR\": \"1.20.1270.60\",\n",
    "    \"ENTH\": \"1.25.40.90\",\n",
    "    \"SH2\": \"3.30.505.10\",\n",
    "    \"SEC14\": \"3.40.525.10\",\n",
    "    \"START\": \"3.30.530.20\",\n",
    "    \"C2DIS\": \"2.60.120.260\",\n",
    "    \"GLA\":\"2.40.20.10\",\n",
    "    \"PLD\":\"3.20.20.190\",\n",
    "    \"PLA\":\"1.20.90.10\",\n",
    "    \"ANNEXIN\":\"1.10.220.10\",\n",
    "}\n",
    "\n",
    "DOMAIN_INTERPRO = {\n",
    "    \"PH\": \"SSF50729\",\n",
    "    \"C2\": \"SSF49562\",\n",
    "    \"C1\": None,\n",
    "    \"PX\": \"SSF64268\",\n",
    "    \"FYVE\": \"SSF57903\", #badly classified it looks like...\n",
    "    \"BAR\": \"SSF103657\",\n",
    "    \"ENTH\": \"SSF48464\",\n",
    "    \"SH2\": \"SSF55550\",\n",
    "    \"SEC14\": [\"SSF52087\",\"SSF46938\"], #the CRAL TRIO domain is truncated in SSF.\n",
    "    \"START\": \"SSF55961\",\n",
    "    \"C2DIS\": \"SSF49785\",\n",
    "    \"GLA\":None,\n",
    "    \"PLD\":\"SSF51695\",\n",
    "    \"PLA\":\"G3DSA:1.20.90.10\",\n",
    "    \"ANNEXIN\":\"SSF47874\",\n",
    "}\n",
    "\n",
    "DOMAIN_INTERPRO_REFINE = {\n",
    "    \"PH\": True,\n",
    "    \"C2\": False,\n",
    "    \"C1\": False,\n",
    "    \"PX\": True,\n",
    "    \"FYVE\": False,\n",
    "    \"BAR\": False,\n",
    "    \"ENTH\": False,\n",
    "    \"SH2\": False,\n",
    "    \"SEC14\": False,\n",
    "    \"START\": True,\n",
    "    \"C2DIS\": False,\n",
    "    \"GLA\":False,\n",
    "    \"PLD\":False,\n",
    "    \"PLA\":True,\n",
    "    \"ANNEXIN\":False,\n",
    "}\n",
    "\n",
    "# Invert keys and values to have CATHID ==> DOMAIN\n",
    "CATH_DOMAIN = {v: k for k, v in DOMAIN_CATH.items()}\n",
    "SUPERFAMILY = CATH_DOMAIN\n",
    "SETUP[\"DOMAIN_PROSITE\"] = DOMAIN_PROSITE\n",
    "SETUP[\"PROSITE_DOMAIN\"] = PROSITE_DOMAIN\n",
    "SETUP[\"DOMAIN_CATH\"] = DOMAIN_CATH\n",
    "SETUP[\"CATH_DOMAIN\"] = CATH_DOMAIN\n",
    "SETUP[\"SUPERFAMILY\"] = SUPERFAMILY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba2ec29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user_stel/AISB/Project/databases/prosite/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROSITEFOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40003074",
   "metadata": {},
   "source": [
    "# Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50053146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from urllib.error import URLError\n",
    "\n",
    "\n",
    "def selectUniquePerCluster(df, cathCluster, Uniref, withAlignment = True):\n",
    "    \"\"\"\n",
    "    Return a datasert with only 1 data per choosed clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    if cathCluster not in [\"S35\",\"S60\",\"S95\",\"S100\"]:\n",
    "        raise ValueError('CathCluster given not in [\"S35\",\"S60\",\"S95\",\"S100\"]')\n",
    "    \n",
    "    if Uniref not in [\"uniref50\",\"uniref90\",\"uniref100\"]:\n",
    "        raise ValueError('CathCluster given not in [\"uniref50\",\"uniref90\",\"uniref100\"]')\n",
    "    \n",
    "    if withAlignment:\n",
    "        df = df[~df.alignment_position.isnull()]\n",
    "    \n",
    "    cathdf = df.query(\"data_type == 'cathpdb'\")\n",
    "    seqdf = df.query(\"data_type == 'prosite'\")\n",
    "    \n",
    "    def selectUniqueCath(group):\n",
    "        uniqueNames = group.cathpdb.unique()\n",
    "        select = uniqueNames[0]\n",
    "        \n",
    "        #return group.query(\"cathpdb == @select\")\n",
    "        return select\n",
    "    \n",
    "    def selectUniqueUniref(group,exclusion):\n",
    "        uniqueNames = group.uniprot_acc.unique()\n",
    "        select = uniqueNames[0]\n",
    "        #return group.query(\"uniprot_acc == @select\")\n",
    "        if select not in exclusion:\n",
    "            return select\n",
    "        \n",
    "\n",
    "    dfReprCathNames = cathdf.groupby([\"domain\",cathCluster]).apply(selectUniqueCath).to_numpy()\n",
    "    \n",
    "    excludeUniref = df.query(\"cathpdb in @dfReprCathNames\").uniprot_acc.unique() #Structures are prior to sequences.\n",
    "    dfReprUnirefNames = seqdf.groupby([\"domain\",Uniref]).apply(selectUniqueUniref, exclusion=excludeUniref).to_numpy()\n",
    "    dfReprCath = cathdf.query(\"cathpdb in @dfReprCathNames\")\n",
    "    dfReprUniref = seqdf.query(\"uniprot_acc in @dfReprUnirefNames\")\n",
    "    \n",
    "    return (pd.concat([dfReprCath,dfReprUniref]))\n",
    "\n",
    "# download AlphaFold models \n",
    "def fetch_pdb_alfafold(uniprotids, domain):\n",
    "    nomodels=[]\n",
    "    withmodels=[]\n",
    "    outfolder = f\"{ALPHAFOLDFOLDER}/{domain}/raw\"\n",
    "    if not os.path.exists(outfolder):\n",
    "        os.makedirs(outfolder)\n",
    "        \n",
    "    extractedfolder = f\"{ALPHAFOLDFOLDER}/{domain}/extracted\"\n",
    "    if not os.path.exists(extractedfolder):\n",
    "        os.makedirs(extractedfolder)\n",
    "    else:\n",
    "        if REBUILD == True: #delete extracted files\n",
    "            files = glob.glob(f\"{extractedfolder}/*.pdb\")\n",
    "            for f in files:\n",
    "                os.remove(f)\n",
    "    \n",
    "    jsonfolder = f\"{ALPHAFOLDFOLDER}/{domain}/json\"\n",
    "    if not os.path.exists(jsonfolder):\n",
    "        os.makedirs(jsonfolder)\n",
    "\n",
    "    for uniprot_id in tqdm(uniprotids, desc=\"Downloading \"):\n",
    "        url = f\"https://alphafold.ebi.ac.uk/files/AF-{uniprot_id}-F1-model_v1.pdb\"\n",
    "        destination = f\"{outfolder}/{uniprot_id}.pdb\"\n",
    "        if not os.path.isfile(destination): \n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, destination)\n",
    "            except urllib.error.HTTPError as err:\n",
    "                nomodels.append(uniprot_id)\n",
    "                continue\n",
    "        withmodels.append(uniprot_id)\n",
    "\n",
    "    \n",
    "    print(f\"{len(nomodels)} out of {len(uniprotids)} without alfafold2 models ({len(nomodels)/len(uniprotids)*100:.2f}%)\")\n",
    "    return withmodels,nomodels\n",
    "\n",
    "def get_prosite_boundaries_dict(domain):\n",
    "    boundaries = {}\n",
    "    prosite_ids = DOMAIN_PROSITE[domain]\n",
    "    if type(prosite_ids) != type([]):\n",
    "        prosite_ids = [prosite_ids]\n",
    "    for msafile in prosite_ids:\n",
    "        msafilepath = f\"{PROSITEFOLDER}/msa/{msafile}.msa\"\n",
    "        msa = AlignIO.read(msafilepath,'fasta')\n",
    "        for record in msa:\n",
    "            seqid = record.id\n",
    "            match = REGEX.match(seqid)\n",
    "            if match:\n",
    "                uniprot_id = match.group(2)\n",
    "                start = match.group(3)\n",
    "                end = match.group(4)\n",
    "                boundaries[uniprot_id] = (int(start),int(end))\n",
    "    return boundaries\n",
    "\n",
    "def get_json(uniprot_acc, domain, source='ssf'):\n",
    "    def request_URL(link, trial=1):\n",
    "        try:\n",
    "            response = requests.get(link).text\n",
    "            return response\n",
    "        except URLError as e:\n",
    "            print(e, link)\n",
    "            if trial >3 :\n",
    "                print('3rd fail, skipping this one.')\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"Trial {trial}, waiting 10s and trying again\")\n",
    "                sleep(10)\n",
    "                return request_URL(link, trial=trial+1)\n",
    "            \n",
    "            \n",
    "    jsonfolder = f\"{ALPHAFOLDFOLDER}/{domain}/json\"\n",
    "    if not os.path.exists(jsonfolder):\n",
    "        os.makedirs(jsonfolder)\n",
    "        \n",
    "    jsonfile = f\"{jsonfolder}/{uniprot_acc}.json\"\n",
    "    if os.path.isfile(jsonfile):\n",
    "        f = open(jsonfile)\n",
    "        interpro = json.load(f)\n",
    "    else:\n",
    "        #make the query on ebi/interpro\n",
    "        response = request_URL(f\"https://www.ebi.ac.uk/interpro/api/entry/{source}/protein/reviewed/{uniprot_acc}/?page_size=200\")\n",
    "        if response == None:\n",
    "            return None\n",
    "        try:\n",
    "            interpro = json.loads(response)\n",
    "        except:\n",
    "            print(f\"no data for {uniprot_acc}.\")\n",
    "            return None\n",
    "        with open(jsonfile,'w') as out:\n",
    "            json.dump(interpro, out, indent=2)\n",
    "            \n",
    "    return(interpro)\n",
    "\n",
    "def get_domain_fragment_query(uniprot_acc, domain, boundaries_prosite):\n",
    "    start_PS,end_PS = boundaries_prosite[uniprot_acc]\n",
    "    starts_ends = [boundaries_prosite[uniprot_acc]]\n",
    "\n",
    "    if DOMAIN_INTERPRO_REFINE[domain] == True:\n",
    "        if domain == \"PLA\":\n",
    "            source = 'cathgene3d'\n",
    "        else:\n",
    "            source = 'ssf'\n",
    "        interpro = get_json(uniprot_acc, domain, source)\n",
    "        if interpro == None:\n",
    "            return None\n",
    "        QueryString = None\n",
    "        \n",
    "        for result in interpro[\"results\"]:\n",
    "            if result[\"metadata\"][\"accession\"] == DOMAIN_INTERPRO[domain]:\n",
    "                entry_protein_locations = result[\"proteins\"][0][\"entry_protein_locations\"]\n",
    "                for entry in entry_protein_locations: #Get the number of truncation in the domain.\n",
    "                    nfrag = len(entry['fragments'])\n",
    "                    \n",
    "                    if domain == 'PLA': #Special case for PLA, we will ignore PROSITE annotation that are actually wrong.\n",
    "                        frag = entry['fragments'][0] #Get first monomer only\n",
    "                        s = entry['fragments'][0].get('start')\n",
    "                        e = entry['fragments'][0].get('end')\n",
    "                        starts_ends = [[s,e]]\n",
    "                    else:\n",
    "                        if nfrag >= 2 and ( entry['fragments'][0].get('start') - 50 <= start_PS <= entry['fragments'][0].get('start')+50) : #if truncated domain AND correspond to the prosite domain\n",
    "                            print(f\"splitting {domain}-{uniprot_acc}\")\n",
    "                            queries = []\n",
    "                            starts_ends = []\n",
    "                            for frag in entry['fragments']:\n",
    "                                s=int(frag.get(\"start\"))\n",
    "                                e=int(frag.get(\"end\"))\n",
    "                                starts_ends.append([s,e])\n",
    "                            if use_uniprot_boundaries == True:\n",
    "                                starts_ends[0][0] = start_PS\n",
    "                                starts_ends[-1][-1] = end_PS\n",
    "\n",
    "                        else: #use prosite fragment\n",
    "                            starts_ends = [[start_PS, end_PS]]\n",
    "                    \n",
    "\n",
    "                QueryString = \" or \".join([f\"({x} <= residue_number <= {y})\" for x,y in starts_ends])\n",
    "        \n",
    "    else:\n",
    "        QueryString = \" or \".join([f\"({x} <= residue_number <= {y})\" for x,y in starts_ends])\n",
    "    \n",
    "    return QueryString\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e55f8",
   "metadata": {},
   "source": [
    "# Download CATH-domain-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c6afb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Domain  Class  Architecture  Topology  Homologous  S35  S60  S95  \\\n",
      "0       1oaiA00      1            10         8          10    1    1    1   \n",
      "1       1go5A00      1            10         8          10    1    1    1   \n",
      "2       3frhA01      1            10         8          10    2    1    1   \n",
      "3       3friA01      1            10         8          10    2    1    1   \n",
      "4       3b89A01      1            10         8          10    2    1    1   \n",
      "...         ...    ...           ...       ...         ...  ...  ...  ...   \n",
      "434852  2kn1A00      4            10      1290          10    2    1    1   \n",
      "434853  1vprA01      4            10      1300          10    1    1    1   \n",
      "434854  1vprA02      4            10      1310          10    1    1    1   \n",
      "434855  1jyoE00      4            10      1330          10    1    1    1   \n",
      "434856  1jyoF00      4            10      1330          10    1    1    1   \n",
      "\n",
      "        S100  S100Count  DomSize  Resolution  \n",
      "0          1          1       59         1.0  \n",
      "1          1          2       69       999.0  \n",
      "2          1          1       58         1.2  \n",
      "3          1          2       54         1.8  \n",
      "4          2          1       54         2.6  \n",
      "...      ...        ...      ...         ...  \n",
      "434852     1          1       49       999.0  \n",
      "434853     1          1      107         1.8  \n",
      "434854     1          1       83         1.8  \n",
      "434855     1          1      102         1.9  \n",
      "434856     1          2      104         1.9  \n",
      "\n",
      "[434857 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "# ─── 1) Settings ───────────────────────────────────────────────────────────────\n",
    "UPDATE     = False\n",
    "CATHFOLDER = \"/home/user_stel/AISB/Project/databases/cath/\"\n",
    "domfile    = \"cath-domain-list-v4_2_0.txt\"\n",
    "url        = (\n",
    "    \"ftp://orengoftp.biochem.ucl.ac.uk/\"\n",
    "    \"cath/releases/all-releases/v4_2_0/\"\n",
    "    \"cath-classification-data/cath-domain-list-v4_2_0.txt\"\n",
    ")\n",
    "\n",
    "# ─── 2) Ensure folder exists ───────────────────────────────────────────────────\n",
    "os.makedirs(CATHFOLDER, exist_ok=True)\n",
    "\n",
    "# ─── 3) Download (if needed) ──────────────────────────────────────────────────\n",
    "destination = Path(CATHFOLDER) / domfile\n",
    "if not destination.exists() or UPDATE:\n",
    "    print(f\"↓ Downloading CATH domain list to {destination}\")\n",
    "    urllib.request.urlretrieve(url, destination)\n",
    "\n",
    "# ─── 4) Read into pandas ───────────────────────────────────────────────────────\n",
    "column_names = [\n",
    "    'Domain','Class','Architecture','Topology','Homologous',\n",
    "    'S35','S60','S95','S100','S100Count','DomSize','Resolution'\n",
    "]\n",
    "\n",
    "df_cath = pd.read_csv(\n",
    "    destination,\n",
    "    sep=r'\\s+',        # whitespace-delimited\n",
    "    header=None,\n",
    "    names=column_names,\n",
    "    comment='#',       # skip lines beginning with “#”\n",
    "    engine='python'\n",
    ")\n",
    "\n",
    "print(df_cath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8c7ab",
   "metadata": {},
   "source": [
    "### Download Correspondance between Uniprot and PDB code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a626a31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Already downloaded: /home/user_stel/AISB/Project/databases/cath/pdb_chain_uniprot.csv.gz\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "from time import sleep\n",
    "\n",
    "url = \"ftp://ftp.ebi.ac.uk/pub/databases/msd/sifts/flatfiles/csv/pdb_chain_uniprot.csv.gz\"\n",
    "destination = os.path.join(CATHFOLDER, \"pdb_chain_uniprot.csv.gz\")\n",
    "\n",
    "def download_with_retries(url, dest, max_tries=3, chunk_size=1024*1024):\n",
    "    for attempt in range(1, max_tries+1):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"↻ Attempt {attempt} failed: {e}\")\n",
    "            if attempt == max_tries:\n",
    "                raise\n",
    "            sleep(5)\n",
    "\n",
    "# download only if missing or forced\n",
    "if not os.path.exists(destination) or UPDATE:\n",
    "    download_with_retries(url, destination)\n",
    "    print(\"✅ Download complete:\", destination)\n",
    "else:\n",
    "    print(\"✔️  Already downloaded:\", destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "836abaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: EOFError raised, file may be slightly truncated but proceeding anyway.\n",
      "✅ Decompressed to: /home/user_stel/AISB/Project/databases/uniprot/pdb_chain_uniprot.csv\n"
     ]
    }
   ],
   "source": [
    "import gzip, shutil\n",
    "\n",
    "gz_path  = \"/home/user_stel/AISB/Project/databases/uniprot/pdb_chain_uniprot.csv.gz\"\n",
    "csv_path = gz_path[:-3]\n",
    "\n",
    "try:\n",
    "    with gzip.open(gz_path, \"rb\") as f_in, open(csv_path, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "except EOFError:\n",
    "    print(\"⚠️ Warning: EOFError raised, file may be slightly truncated but proceeding anyway.\")\n",
    "\n",
    "print(\"✅ Decompressed to:\", csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb412a3c",
   "metadata": {},
   "source": [
    "# Download Prosite files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f3a12c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PROSITE data already present in /home/user_stel/AISB/Project/databases/prosite/msa\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request, tarfile\n",
    "\n",
    "PROSITE_URL      = \"ftp://ftp.expasy.org/databases/prosite/prosite_alignments.tar.gz\"\n",
    "PROSITEFOLDER    = \"/home/user_stel/AISB/Project/databases/prosite/\"\n",
    "archive_path     = os.path.join(PROSITEFOLDER, \"prosite_alignments.tar.gz\")\n",
    "prosite_alignments       = os.path.join(PROSITEFOLDER, \"msa\")\n",
    "\n",
    "# Only download & extract if the msa folder doesn’t already exist:\n",
    "if not os.path.isdir(prosite_alignments):\n",
    "    print(f\"↓ Downloading PROSITE alignments to {archive_path}\")\n",
    "    urllib.request.urlretrieve(PROSITE_URL, archive_path)\n",
    "\n",
    "    print(\"→ Extracting…\")\n",
    "    with tarfile.open(archive_path, \"r:gz\") as tf:\n",
    "        tf.extractall(path=PROSITEFOLDER)\n",
    "\n",
    "    # Rename the extracted folder to “msa”\n",
    "    os.rename(\n",
    "        os.path.join(PROSITEFOLDER, \"prosite_alignments\"),\n",
    "        prosite_alignments\n",
    "    )\n",
    "\n",
    "    # Clean up\n",
    "    os.remove(archive_path)\n",
    "    print(\"✅ PROSITE data ready in\", prosite_alignments)\n",
    "else:\n",
    "    print(\"✅ PROSITE data already present in\", prosite_alignments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69496a9c",
   "metadata": {},
   "source": [
    "# Download CATH PDB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6279d087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef999f48f69146fcb86741472350097e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/434857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062b737531bc4c0f8032edecfd14d05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ANNEXIN:   0%|          | 0/405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457f1c9671ee4a52ae8cd1b241b67f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BAR:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f18e174df2c44a7810745153a2c69bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PLA:   0%|          | 0/505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6af70124fb400eae1af4afebb0d259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ENTH:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1d8875360042a9b54f37a5ddf7e9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PH:   0%|          | 0/818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5891d2c31a30456982c4b395bea7fcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GLA:   0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda831b025174c88b5b1ee326613b8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C2DIS:   0%|          | 0/1295 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5877cb6bf34cff83b3b824393d0701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C2:   0%|          | 0/346 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ad14eb9bb84b6186a5a4c825beda34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PLD:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755a49d69bb24aa193a8c8fb83aa5244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PX:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b705673b66a242218a32bd92f02c2f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FYVE:   0%|          | 0/605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cd5aeee99144d882b8dafddb0a4014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SH2:   0%|          | 0/661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51a7efd4fc541be82168ee1ca858089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "START:   0%|          | 0/445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50658b9f146405088c3005fd26f5bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C1:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0e42bb8d664d65a0496eb27c1f5bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SEC14:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "domfile = os.path.join(CATHFOLDER, \"cath-domain-list-v4_2_0.txt\")\n",
    "column_names = [\n",
    "    'Domain','Class','Architecture','Topology','Homologous',\n",
    "    'S35','S60','S95','S100','S100Count','DomSize','resolution',\n",
    "]\n",
    "df_domains = pd.read_csv(\n",
    "    domfile,\n",
    "    comment='#', sep=r\"\\s+\", header=None, names=column_names, engine='python'\n",
    ")\n",
    "df_domains['Superfamily'] = df_domains.progress_apply(\n",
    "    lambda x: f\"{x.Class}.{x.Architecture}.{x.Topology}.{x.Homologous}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ─── 2) Group your domains by superfamily ────────────────────────────────────\n",
    "domains_per_sf = (\n",
    "    df_domains\n",
    "      .groupby('Superfamily')['Domain']\n",
    "      .apply(list)\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "# ─── 3) Download function ────────────────────────────────────────────────────\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1) Your mapping of superfamily name → CATH class.arch.topo.homologous\n",
    "DOMAIN_CATH = {\n",
    "    \"PH\":     \"2.30.29.30\",  # no\n",
    "    \"C2\":     \"2.60.40.150\", # no\n",
    "    \"C1\":     \"3.30.60.20\",  # no \n",
    "    \"PX\":     \"3.30.1520.10\",# no\n",
    "    \"FYVE\":   \"3.30.40.10\",  # no\n",
    "    \"BAR\":    \"1.20.1270.60\",# yes membrane-binding, signaling\n",
    "    \"ENTH\":   \"1.25.40.90\",  # yes signaling \n",
    "    \"SH2\":    \"3.30.505.10\", # yes metabolism, growth\n",
    "    \"SEC14\":  \"3.40.525.10\", # yes metabolism, transfering, energy balance \n",
    "    \"START\":  \"3.30.530.20\", # no\n",
    "    \"C2DIS\":  \"2.60.120.260\",# no\n",
    "    \"GLA\":    \"2.40.20.10\",  # yes signaling\n",
    "    \"PLD\":    \"3.20.20.190\", # no\n",
    "    \"PLA\":    \"1.20.90.10\",  # no\n",
    "    \"ANNEXIN\":\"1.10.220.10\", # no\n",
    "}\n",
    "\n",
    "# 2) Read the domain list and compute a “Superfamily” column\n",
    "domfile = \"/home/user_stel/AISB/Project/databases/cath/cath-domain-list-v4_2_0.txt\"\n",
    "cols   = ['Domain','Class','Architecture','Topology','Homologous',\n",
    "          'S35','S60','S95','S100','S100Count','DomSize','Resolution']\n",
    "df_domains     = pd.read_csv(domfile, sep=r'\\s+', header=None, names=cols, comment='#', engine='python')\n",
    "df_domains['Superfamily'] = (\n",
    "    df_domains.Class.astype(str) + '.' +\n",
    "    df_domains.Architecture.astype(str) + '.' +\n",
    "    df_domains.Topology.astype(str) + '.' +\n",
    "    df_domains.Homologous.astype(str)\n",
    ")\n",
    "\n",
    "# 3) Keep only the rows whose Superfamily is in our DOMAIN_CATH values\n",
    "wanted = set(DOMAIN_CATH.values())\n",
    "df_domains = df_domains[df_domains.Superfamily.isin(wanted)]\n",
    "\n",
    "# 4) Invert DOMAIN_CATH so we can look up the name from the string\n",
    "inv = {v:k for k,v in DOMAIN_CATH.items()}\n",
    "\n",
    "# 5) Build a dict superfamily → list of domain IDs\n",
    "domains_per_sf = (\n",
    "    df_domains.groupby('Superfamily')\n",
    "      .Domain\n",
    "      .apply(list)\n",
    "      .rename(index=inv)   # turn the index from the string back to PH, C2, etc.\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "# 6) For each superfamily, download *only* those domains\n",
    "CATHFOLDER = \"/home/user_stel/AISB/Project/databases/cath/\"\n",
    "for sf, domlist in domains_per_sf.items():\n",
    "    raw_dir   = Path(CATHFOLDER) / \"domains\" / sf / \"raw\"\n",
    "    clean_dir = Path(CATHFOLDER) / \"domains\" / sf / \"cleaned\"\n",
    "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "    clean_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for dom in tqdm(domlist, desc=sf):\n",
    "        url  = f\"http://www.cathdb.info/version/v4_2_0/api/rest/id/{dom}.pdb\"\n",
    "        dst  = raw_dir / f\"{dom}.pdb\"\n",
    "        if not dst.exists():\n",
    "            urllib.request.urlretrieve(url, dst.as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bf28eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc708516f9454fe4a0fd83b67f421c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=54358), Label(value='0 / 54358')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading Cath domain list\n",
    "df_domains = pd.read_csv(domfile,comment='#', sep=r\"\\s+\", header=None)\n",
    "df_domains.columns = column_names\n",
    "if PARRALEL:\n",
    "    df_domains['Superfamily'] = df_domains.parallel_apply(lambda x: f\"{x.Class}.{x.Architecture}.{x.Topology}.{x.Homologous}\", axis=1)\n",
    "else:\n",
    "    df_domains['Superfamily'] = df_domains.progress_apply(lambda x: f\"{x.Class}.{x.Architecture}.{x.Topology}.{x.Homologous}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2120151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Domain  Class  Architecture  Topology  Homologous  S35  S60  S95  \\\n",
      "0       1oaiA00      1            10         8          10    1    1    1   \n",
      "1       1go5A00      1            10         8          10    1    1    1   \n",
      "2       3frhA01      1            10         8          10    2    1    1   \n",
      "3       3friA01      1            10         8          10    2    1    1   \n",
      "4       3b89A01      1            10         8          10    2    1    1   \n",
      "...         ...    ...           ...       ...         ...  ...  ...  ...   \n",
      "434852  2kn1A00      4            10      1290          10    2    1    1   \n",
      "434853  1vprA01      4            10      1300          10    1    1    1   \n",
      "434854  1vprA02      4            10      1310          10    1    1    1   \n",
      "434855  1jyoE00      4            10      1330          10    1    1    1   \n",
      "434856  1jyoF00      4            10      1330          10    1    1    1   \n",
      "\n",
      "        S100  S100Count  DomSize  resolution   Superfamily  \n",
      "0          1          1       59         1.0     1.10.8.10  \n",
      "1          1          2       69       999.0     1.10.8.10  \n",
      "2          1          1       58         1.2     1.10.8.10  \n",
      "3          1          2       54         1.8     1.10.8.10  \n",
      "4          2          1       54         2.6     1.10.8.10  \n",
      "...      ...        ...      ...         ...           ...  \n",
      "434852     1          1       49       999.0  4.10.1290.10  \n",
      "434853     1          1      107         1.8  4.10.1300.10  \n",
      "434854     1          1       83         1.8  4.10.1310.10  \n",
      "434855     1          1      102         1.9  4.10.1330.10  \n",
      "434856     1          2      104         1.9  4.10.1330.10  \n",
      "\n",
      "[434857 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating the superfamily \n",
    "cathSuperFamily = pd.DataFrame()\n",
    "cathSuperFamily['Superfamily'] = df_domains.Superfamily\n",
    "cathSuperFamily['Domain'] = df_domains.Domain\n",
    "print(df_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0865fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATHVERSION = 'v4_2_0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e909a",
   "metadata": {},
   "source": [
    "# Download AlphaFold Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abb6efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2353 UniProt IDs in PROSITE MSAs\n"
     ]
    }
   ],
   "source": [
    "### Parse your PROSITE MSAs for UniProt IDs \n",
    "import re, glob\n",
    "from Bio import SeqIO\n",
    "\n",
    "PROSITE_MSA_DIR = \"/home/user_stel/AISB/Project/databases/prosite/msa\"\n",
    "PROSITE_HEADER_RX = re.compile(r\"^>[^|]+\\|([^/]+)/\")\n",
    "\n",
    "def get_uniprot_from_msa(msa_path):\n",
    "    # grab just the first header line\n",
    "    with open(msa_path) as fh:\n",
    "        for line in fh:\n",
    "            if line.startswith(\">\"):\n",
    "                m = PROSITE_HEADER_RX.match(line)\n",
    "                return m.group(1)\n",
    "    return None\n",
    "\n",
    "prosite_ids = []\n",
    "for msa in glob.glob(f\"{PROSITE_MSA_DIR}/*.msa\"):\n",
    "    uid = get_uniprot_from_msa(msa)\n",
    "    if uid:\n",
    "        prosite_ids.append(uid)\n",
    "\n",
    "prosite_ids = list(set(prosite_ids))\n",
    "print(f\"Found {len(prosite_ids)} UniProt IDs in PROSITE MSAs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d2e65b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46780 UniProt IDs in CATH mapping\n"
     ]
    }
   ],
   "source": [
    "# Read your CATH→UniProt mapping\n",
    "import pandas as pd\n",
    "\n",
    "mapping_csv = \"/home/user_stel/AISB/Project/databases/cath/pdb_chain_uniprot.csv\"\n",
    "m = pd.read_csv(mapping_csv, comment=\"#\")\n",
    "cath_ids = m[\"SP_PRIMARY\"].dropna().unique().tolist()\n",
    "print(f\"Found {len(cath_ids)} UniProt IDs in CATH mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df23ab74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique UniProt IDs to fetch from AlphaFold: 48770\n"
     ]
    }
   ],
   "source": [
    "# combine both\n",
    "uniprot_ids = list(set(prosite_ids) | set(cath_ids))\n",
    "print(f\"Total unique UniProt IDs to fetch from AlphaFold: {len(uniprot_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in SUPERFAMILY.keys():\n",
    "    # 1) Download full-length AF models for this domain\n",
    "    withmodels, nomodels = fetch_pdb_alfafold(uniprot_ids, domain=domain)\n",
    "    print(f\"[{domain}] fetched {len(withmodels)} / {len(withmodels)+len(nomodels)}\")\n",
    "\n",
    "    # 2) Build the slice query for this domain\n",
    "    boundaries = get_prosite_boundaries_dict(domain)\n",
    "    query = get_domain_fragment_query(domain, boundaries)\n",
    "\n",
    "    # 3) Trim & save each model\n",
    "    raw_dir       = f\"{ALPHAFOLDFOLDER}/{domain}/raw\"\n",
    "    extracted_dir = f\"{ALPHAFOLDFOLDER}/{domain}/extracted\"\n",
    "    os.makedirs(extracted_dir, exist_ok=True)\n",
    "\n",
    "    for uid in withmodels:\n",
    "        src = f\"{raw_dir}/{uid}.pdb\"\n",
    "        dst = f\"{extracted_dir}/{uid}.pdb\"\n",
    "        pp = PandasPdb().read_pdb(src)\n",
    "        pp.df[\"ATOM\"] = pp.df[\"ATOM\"].query(query)\n",
    "        pp.to_pdb(dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef4788ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd56af233cd4a569bcab47168fe7322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading :   0%|          | 0/48770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(0 bytes read)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIncompleteRead\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/http/client.py:597\u001b[39m, in \u001b[36mHTTPResponse._read_chunked\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt <= chunk_left:\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     value.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    598\u001b[39m     \u001b[38;5;28mself\u001b[39m.chunk_left = chunk_left - amt\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/http/client.py:642\u001b[39m, in \u001b[36mHTTPResponse._safe_read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) < amt:\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt-\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mIncompleteRead\u001b[39m: IncompleteRead(1283 bytes read, 6909 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mIncompleteRead\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m use_all_AFmodels = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 2️⃣ Download full-length AF PDBs:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m withmodels, nomodels = \u001b[43mfetch_pdb_alfafold\u001b[49m\u001b[43m(\u001b[49m\u001b[43muniprot_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPH\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(withmodels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m models fetched, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(nomodels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m missing\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 3️⃣ Compute “in-domain” slice query:\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mfetch_pdb_alfafold\u001b[39m\u001b[34m(uniprotids, domain)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(destination): \n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m         \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     74\u001b[39m         nomodels.append(uniprot_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/urllib/request.py:268\u001b[39m, in \u001b[36murlretrieve\u001b[39m\u001b[34m(url, filename, reporthook, data)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reporthook:\n\u001b[32m    266\u001b[39m     reporthook(blocknum, bs, size)\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m block := \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    269\u001b[39m     read += \u001b[38;5;28mlen\u001b[39m(block)\n\u001b[32m    270\u001b[39m     tfp.write(block)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/http/client.py:473\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m         \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/http/client.py:607\u001b[39m, in \u001b[36mHTTPResponse._read_chunked\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(value)\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(value)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mIncompleteRead\u001b[39m: IncompleteRead(0 bytes read)"
     ]
    }
   ],
   "source": [
    "REBUILD = True\n",
    "use_uniprot_boundaries = True\n",
    "use_all_AFmodels = True\n",
    "\n",
    "# 2️⃣ Download full-length AF PDBs:\n",
    "withmodels, nomodels = fetch_pdb_alfafold(uniprot_ids, domain=\"PH\")\n",
    "print(f\"✅ {len(withmodels)} models fetched, {len(nomodels)} missing\")\n",
    "\n",
    "# 3️⃣ Compute “in-domain” slice query:\n",
    "boundaries_prosite = get_prosite_boundaries_dict(\"PH\")\n",
    "query = get_domain_fragment_query(\"PH\", boundaries_prosite)\n",
    "#    e.g. \"(45 <= residue_number <= 130) or (200 <= residue_number <= 270)\"\n",
    "\n",
    "raw_dir      = f\"{ALPHAFOLDFOLDER}/PH/raw\"\n",
    "extracted_dir= f\"{ALPHAFOLDFOLDER}/PH/extracted\"\n",
    "\n",
    "os.makedirs(extracted_dir, exist_ok=True)\n",
    "\n",
    "for uid in withmodels:\n",
    "    src = f\"{raw_dir}/{uid}.pdb\"\n",
    "    dst = f\"{extracted_dir}/{uid}.pdb\"\n",
    "    pp = PandasPdb().read_pdb(src)\n",
    "    pp.df[\"ATOM\"] = pp.df[\"ATOM\"].query(query)\n",
    "    pp.to_pdb(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63481c",
   "metadata": {},
   "source": [
    "# Generation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "795dc87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECALCULATION = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cd6b5fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412f9d6715d443609fe9b1e2198f7497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=True, description='Recalculation ?', icon='cogs', tooltip='Click for recalculation')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recalculation_widget = widgets.ToggleButton(\n",
    "    value=RECALCULATION,\n",
    "    description='Recalculation ?',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click for recalculation',\n",
    "    icon='cogs' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "display(recalculation_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd0f9b",
   "metadata": {},
   "source": [
    "### Instanciate the builder object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be7f8774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pepr2ds.builder.Builder' from '/home/user_stel/AISB/Project/pepr2ds/builder/Builder.py'>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pepr2ds.builder.Builder as builderEngine\n",
    "importlib.reload(builderEngine)\n",
    "builder = builderEngine.Builder(SETUP, recalculate = recalculation_widget.value, update=False, notebook = True, core=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69e87b",
   "metadata": {},
   "source": [
    "### Cleaning -> Populate the cleaned directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bcecedb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0024e8e6e1419ca3964ce92383d129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaning:   0%|          | 0/5916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "builder.structure.clean_all_pdbs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad350039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5916 raw PDBs and 5916 cleaned PDBs.\n"
     ]
    }
   ],
   "source": [
    "# Verify that the **builder.structure.clean_all_pdbs()** worked \n",
    "\n",
    "raw = glob.glob(os.path.join(SETUP[\"CATHFOLDER\"], \"domains\", \"*\", \"raw\", \"*.pdb\"))\n",
    "cleaned = glob.glob(os.path.join(SETUP[\"CATHFOLDER\"], \"domains\", \"*\", \"cleaned\", \"*.pdb\"))\n",
    "\n",
    "print(f\"Found {len(raw)} raw PDBs and {len(cleaned)} cleaned PDBs.\")\n",
    "\n",
    "# it should display same number or raw PDBs and cleaned ones \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33164a86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df85442d",
   "metadata": {},
   "source": [
    "### Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bb891",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_struc = builder.structure.build_structural_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
